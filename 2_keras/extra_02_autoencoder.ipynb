{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of generated images (reversed grayscale)\n",
    "def show_plot(examples, n, with_channel=True):\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        if with_channel:\n",
    "            # shape = (n_sample, x_axis, y_axis, channel)\n",
    "            plt.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "        else:\n",
    "            # shape = (n_sample, x_axis, y_axis)\n",
    "            plt.imshow(examples[i], cmap='gray_r')\n",
    "            \n",
    "    plt.show()\n",
    "    \n",
    "def t_sne_visualize(x, y):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    tsne_obj = tsne.fit_transform(x)\n",
    "\n",
    "    tsne_df = pd.DataFrame({'X': tsne_obj[:, 0],\n",
    "                            'Y': tsne_obj[:, 1],\n",
    "                            'classes': y})\n",
    "    sns.scatterplot(x=\"X\", y=\"Y\",\n",
    "                    hue=\"classes\",\n",
    "                    legend='full',\n",
    "                    size=0.5,\n",
    "                    alpha=0.2,\n",
    "                    data=tsne_df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# data pre-processing\n",
    "# flat the input data\n",
    "x_train = x_train.reshape(*x_train.shape, 1) / 255.    # normalize\n",
    "x_test = x_test.reshape(*x_test.shape, 1) / 255.      # normalize\n",
    "\n",
    "# one-hot output data\n",
    "# y_train = to_categorical(y_train, num_classes=10)\n",
    "# y_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trival autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=30,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "\n",
    "# encoded_imgs = encoder.predict(x_test)\n",
    "# decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convolutional autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, \n",
    "                          Conv2DTranspose, BatchNormalization, Flatten, Reshape)\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=30,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n + 1 )\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convolutional autoencoder (version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ENCODER\n",
    "inp = Input((28, 28, 1))\n",
    "e = Conv2D(32, (3, 3), activation='relu')(inp)\n",
    "e = MaxPooling2D((2, 2))(e)\n",
    "e = Conv2D(64, (3, 3), activation='relu')(e)\n",
    "e = MaxPooling2D((2, 2))(e)\n",
    "e = Conv2D(64, (3, 3), activation='relu')(e)\n",
    "l = Flatten()(e)\n",
    "\n",
    "# l = Dense(49, activation='relu')(l)\n",
    "l = Dense(49, activation='softmax')(l)\n",
    "\n",
    "encoder = Model(inp, l)\n",
    "\n",
    "\n",
    "# DECODER\n",
    "l = Input((49,))\n",
    "d = Reshape((7, 7, 1))(l)\n",
    "d = Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(d)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(d)\n",
    "\n",
    "decoder = Model(l, decoded)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.0266 - val_loss: 0.1593\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.0117 - val_loss: 0.0108\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0093 - val_loss: 0.0086\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0087 - val_loss: 0.0088\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0082 - val_loss: 0.0224\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0080 - val_loss: 0.0081\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.0077 - val_loss: 0.0130\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0075 - val_loss: 0.0488\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.0073 - val_loss: 0.0141\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0072 - val_loss: 0.0086\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0071 - val_loss: 0.0198\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0069 - val_loss: 0.0406\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0068 - val_loss: 0.0316\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0065 - val_loss: 0.0116\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0064 - val_loss: 0.0444\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0063 - val_loss: 0.0262\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0063 - val_loss: 0.0186\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.0062 - val_loss: 0.0349\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.0061 - val_loss: 0.0324\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0061 - val_loss: 0.0169\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0060 - val_loss: 0.0693\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0060 - val_loss: 0.0342\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.0060 - val_loss: 0.0098\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0059 - val_loss: 0.0313\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0060 - val_loss: 0.0195\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0059 - val_loss: 0.0115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f1234261d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=30,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n + 1 )\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variational autoencoder (version 1 - old version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Layer, Lambda, Add, Multiply\n",
    "from keras import backend as K\n",
    "from keras.losses import mse, binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
    "\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. we require the sum\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "def nll2(y_true, y_pred):\n",
    "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
    "    \n",
    "    # Why this do not work ??\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. we require the sum\n",
    "    return K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" \n",
    "    This Layer did nothing but add KL loss\n",
    "    \n",
    "    Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = np.prod(x_train.shape[1:])\n",
    "latent_dim = 2\n",
    "intermediate_dim = 512\n",
    "\n",
    "\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "z_mu = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "eps = Input(tensor=K.random_normal(shape=(K.shape(x)[0], latent_dim)))\n",
    "z_eps = Multiply()([z_sigma, eps])\n",
    "z = Add()([z_mu, z_eps])\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "    Dense(original_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "x_pred = decoder(z)\n",
    "\n",
    "encoder = Model(x, z_mu)\n",
    "\n",
    "vae = Model(inputs=[x, eps], outputs=x_pred)\n",
    "vae.compile(optimizer='rmsprop', loss=nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "hist = vae.fit(x_train,\n",
    "        x_train,\n",
    "        shuffle=True,\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        validation_data=(x_test, x_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hist.history).plot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "z_test = encoder.predict(x_test, batch_size=128)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(z_test[:, 0], z_test[:, 1], c=y_test,\n",
    "            alpha=.4, s=3**2, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "\n",
    "# linearly spaced coordinates on the unit square were transformed\n",
    "# through the inverse CDF (ppf) of the Gaussian to produce values\n",
    "# of the latent variables z, since the prior of the latent space\n",
    "# is Gaussian\n",
    "\n",
    "z1 = norm.ppf(np.linspace(0.01, 0.99, n))\n",
    "z2 = norm.ppf(np.linspace(0.01, 0.99, n))\n",
    "z_grid = np.dstack(np.meshgrid(z1, z2))\n",
    "\n",
    "x_pred_grid = decoder.predict(z_grid.reshape(n*n, latent_dim)) \\\n",
    "                     .reshape(n, n, digit_size, digit_size)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(np.block(list(map(list, x_pred_grid))), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (convolutional) variational autoencoder (version2 - new version, base on tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= 28 * 28\n",
    "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.decoder(self.encoder(x)[-1])\n",
    "    \n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(encoder, decoder):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    scale = 2.0\n",
    "    figsize = 15\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    \n",
    "    # start_range = digit_size // 2\n",
    "    # end_range = n * digit_size + start_range + 1\n",
    "    # pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    # sample_range_x = np.round(grid_x, 1)\n",
    "    # sample_range_y = np.round(grid_y, 1)\n",
    "    # plt.xticks(pixel_range, sample_range_x)\n",
    "    # plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congyuml",
   "language": "python",
   "name": "congyuml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
