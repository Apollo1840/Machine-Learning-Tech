{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Concatenate, Flatten, Dropout, Dense\n",
    "from tensorflow.python.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.callbacks import History\n",
    "from lib.KerasHelpers.modelhelpers import model_placement\n",
    "from src.models.model import TextCNN\n",
    "from argparse import ArgumentParser\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "    \n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this first thing is to get the word_embeding matrix (embeding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this funtion is not used here, but recommend to use in real cases\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def get_word_vectors(word_index, word2vec_flag=False):\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "    if not word2vec_flag:\n",
    "\n",
    "        embeddings_index = {}\n",
    "        f = open(os.path.join('data/raw/wordvectors/glove.6B', 'glove.6B.300d.txt'))\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "        for word, i in word_index.items():\n",
    "            if i >= MAX_NUM_WORDS:\n",
    "                continue\n",
    "\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                counter = counter + 1\n",
    "    else:\n",
    "\n",
    "        model = KeyedVectors.load_word2vec_format('data/raw/wordvectors/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "        num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "        for word, i in word_index.items():\n",
    "            if i>=MAX_NUM_WORDS:\n",
    "                continue\n",
    "            try:\n",
    "                embedding_vector = model[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                counter = counter + 1\n",
    "            except KeyError:\n",
    "                embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n",
    "                \n",
    "    print ('Found %s word vectors.' % counter)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "embedding_matrix = get_word_vectors(word_index, word2vec_flag=True)\n",
    "# word2vec true for the google word2vec, false for the glove\n",
    "\n",
    "print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "\n",
    "# fname_wordvec = 'glove_'\n",
    "fname_wordvec = 'word2vec_'\n",
    "\n",
    "np.save(os.path.join(processed_DIR, fname_wordvec+'embedding.npy'), embedding_matrix, allow_pickle=False)\n",
    "\n",
    "print ('Saved embedding.npy and data.h5 in %s' % processed_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chainwise implementation\n",
    "\n",
    "def TextCNN(MAX_SEQUENCE_LENGTH, num_classes, num_words, dropout_rate=0.4, flag='rand', embedding_weights=None):\n",
    "\n",
    "    \"\"\"\n",
    "    - flag: 'rand':embedding layer train from scrath\n",
    "            'non': finetune the predefined embedding layer\n",
    "            others: use predefined embedding layer\n",
    "    \"\"\"\n",
    "    EMBEDDING_DIM = 300\n",
    "        \n",
    "    x_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    if 'rand' in flag:\n",
    "        embedding_layer = Embedding(input_dim=num_words,\n",
    "                                    output_dim=EMBEDDING_DIM,\n",
    "                                    embeddings_initializer='uniform',\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH)\n",
    "        \n",
    "    else:\n",
    "        trainable_flag = False\n",
    "        \n",
    "        if 'non' in flag: trainable_flag = True\n",
    "        \n",
    "        embedding_layer = Embedding(input_dim=num_words,\n",
    "                                    output_dim=EMBEDDING_DIM,\n",
    "                                    weights=[embedding_weights],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=trainable_flag)\n",
    "\n",
    "    x = embedding_layer(x_input)\n",
    "\n",
    "    kernel_sizes = [3, 4, 5]\n",
    "    pooled = []\n",
    "\n",
    "    for kernel in kernel_sizes:\n",
    "\n",
    "        conv = Conv1D(filters=100,\n",
    "                      kernel_size=kernel,\n",
    "                      padding='valid',\n",
    "                      strides=1,\n",
    "                      kernel_initializer='he_uniform',\n",
    "                      activation='relu')(x)\n",
    "        \n",
    "        pool = MaxPooling1D(pool_size=MAX_SEQUENCE_LENGTH - kernel + 1)(conv)\n",
    "\n",
    "        pooled.append(pool)\n",
    "\n",
    "    merged = Concatenate(axis=-1)(pooled)\n",
    "\n",
    "    flatten = Flatten()(merged)\n",
    "\n",
    "    drop = Dropout(rate=dropout_rate)(flatten)\n",
    "    \n",
    "    x_output = Dense(num_classes, kernel_initializer='he_uniform', activation='softmax')(drop)\n",
    "\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_DIR = \n",
    "drop_rate = 0.2\n",
    "flag      = 'rand'\n",
    "wordvectores = False\n",
    "batch_size   = 32\n",
    "num_epochs   = 12\n",
    "lr = 0.001\n",
    "\n",
    "kwargs = {'MAX_SEQUENCE_LENGTH': x_train.shape[1],\n",
    "          'num_classes': y_train.shape[1],\n",
    "          'num_words': data_dict['num_words'],\n",
    "          'dropout_rate': dropout_rate,\n",
    "          'flag': flag\n",
    "         }\n",
    "\n",
    "if 'rand' in mode:\n",
    "    kwargs['embedding_weights'] = None\n",
    "\n",
    "else:\n",
    "    fname_wordvec = 'glove_'\n",
    "\n",
    "    if wordvectors:\n",
    "        fname_wordvec = 'word2vec_'\n",
    "\n",
    "    kwargs['embedding_weights'] = np.load(os.path.join(dataset_DIR, fname_wordvec+'embedding.npy'))\n",
    "\n",
    "text_model = TextCNN(**kwargs)\n",
    "\n",
    "# model = model_placement(text_model, num_gpus=options.num_gpus)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=SGD(lr=lr))\n",
    "\n",
    "model.fit(x=x_train, y=y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=num_epochs, \n",
    "          validation_data=validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congyuml",
   "language": "python",
   "name": "congyuml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
