{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First tutorial on Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this tutorial teaches you how to **create/train/test(evaluate)** a Pytorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: create your pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the first step, you need to know how to create a Pytorch DL model(namely, **nn.Module**):\n",
    "\n",
    "you just need simply create a class inheriant from the nn.Module.\n",
    "\n",
    "This class must have defined: \n",
    "- self.loss\n",
    "- self.optimizer\n",
    "- self.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.loss = None  # or self.criterion, this is a name which is more widely used.\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. loss is easy, you can just got one from the library. eg. nn.CrossEntropyLoss()\n",
    "2. optimizer is also easy, just got on from the library. eg.torch.optim.SGD()\n",
    "    but you need to input self.parameters manually:\n",
    "    > self.optimizer = torch.optim.SGD(self.parameters(), lr=1e-3) m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)  # call by reference\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. lets introduce the .forward function now.\n",
    "\n",
    "here basically you defined your model **architecture**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    y = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.functional.sigmoid, \n",
    "        )(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also defined you blocks in __init__(), or in some other methods, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-408daede72c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "import nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.sigmoid = F.sigmoid\n",
    "\n",
    "        # loss and optimizer\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)  # call by reference\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        y = self.softmax(logits, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. In pytorch, if you nn.CrossEntropyLoss() (or nn.BCEwithLogitsLoss()), no log_softmax() or nn.Sigmoid() is needed. (must **without**) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have your pytorch model, to train it you should \n",
    "define your **train_step()** first.\n",
    "\n",
    "train_step() is how do you treat with your each batch data (x_batch, y_batch).\n",
    "normally, you do calculate the gradients and feed it to the optimizer:\n",
    "\n",
    "- 1) clear the gradient information.\n",
    "- 2) calculate the gradient information and feed it to the parameters.\n",
    "- 3) optimizer(which is connected to the parameters) updates the parameters based on gradient information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(self, x, y):\n",
    "    \n",
    "    # 1) clear previous gradient:\n",
    "    self.optimizer.zero_grad()\n",
    "    \n",
    "    # 2) calculate the gradient:\n",
    "    y_pred = self(x)\n",
    "    loss = self.loss(y_pred, y)\n",
    "    loss.backward()\n",
    "    # regularly (y_pred, y) are also called: (output, target)\n",
    "    \n",
    "    # 3) update the paramters:\n",
    "    self.optimizer.step()\n",
    "    \n",
    "    return self.loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or you could split the logic into following two APIs:\n",
    "- step_function()\n",
    "- backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(self, x, y):\n",
    "    output = self(x)\n",
    "    loss = self.loss(output, y)\n",
    "    return output, loss\n",
    "\n",
    "def backprop(self):\n",
    "    self.optimizer.zero_grad()\n",
    "    self.loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "\n",
    "def train_step(self, x, y):\n",
    "    output, loss = self.step_function(x, y)\n",
    "    self.backprop()\n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this helper function, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Pytorch model\n",
    "model = NeuralNetwork()    \n",
    "\n",
    "# first of all, change to train mode\n",
    "model.train()\n",
    "\n",
    "for t in range(epochs):\n",
    "    for x, y in tqdm(datagen, total=len(datagen)):\n",
    "        model.train_step(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3: evaluate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to evaluate the model, you need:\n",
    "\n",
    "- switch to evaluate mode\n",
    "- turn off gradient calculation\n",
    "- use **.item()** to get the scalars instead of the calculatable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # switch to evaluation mode\n",
    "\n",
    "test_loss, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X, y in datagen:\n",
    "        y_pred = self(X)\n",
    "        test_loss += model.loss(y_pred, y).item()\n",
    "        correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "size = len(datagen.dataset)\n",
    "test_loss_avg = test_loss / size  # avg_loss\n",
    "acc = correct / size  # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use model is simple: you just employ y = model(x).\n",
    "\n",
    "however to accelate it, you are highly suggested to switch to eval mode and turn off gradient calculation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "\n",
    "return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from material.data import mnist_datagen\n",
    "\n",
    "from i_nn import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64 tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "train_gen, test_gen = fmnist_datagen()\n",
    "for x, y in test_gen:\n",
    "    x_sample, y_sample = x[0], y[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for batch_idx, (x, y) in tqdm(enumerate(train_gen), total=len(train_gen)):\n",
    "    net.optimizer.zero_grad()\n",
    "    y_pred = net(x)\n",
    "    print(y.shape)\n",
    "    print(y_pred.shape)\n",
    "    net.loss(y_pred, y).backward()\n",
    "    net.optimizer.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PytorchNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 12/938 [00:00<00:07, 116.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:06<00:00, 136.80it/s]\n",
      "  1%|▏         | 14/938 [00:00<00:06, 132.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.016695 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:06<00:00, 138.21it/s]\n",
      "  1%|▏         | 14/938 [00:00<00:06, 132.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.016114 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:06<00:00, 137.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.015623 \n",
      "\n",
      "Done!\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.015886 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit_datagen(train_gen, epochs=3)\n",
    "model.eval_datagen(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Ankle boot, Actual: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "# use\n",
    "pred = model.predict(x_sample)\n",
    "print('Predicted: {}, Actual: {}'.format(\n",
    "    fmnist_classes[pred[0].argmax(0)], fmnist_classes[y_sample]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congyuml",
   "language": "python",
   "name": "congyuml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
