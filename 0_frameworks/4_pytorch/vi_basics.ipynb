{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.init import normal_\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty x:  tensor([0.0521], requires_grad=True)\n",
      "random x:  tensor([-1.7507], requires_grad=True)\n",
      "y:  tensor([-3.5015], grad_fn=<MulBackward0>)\n",
      "gradient of x before backward:  None\n",
      "gradient of x after backward:  tensor([2.])\n",
      "x before optimize:  tensor([-1.7507], requires_grad=True)\n",
      "y:  tensor([-3.5015], grad_fn=<MulBackward0>)\n",
      "x after optimize:  tensor([-1.8507], requires_grad=True)\n",
      "y:  tensor([-3.7015], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# variable init\n",
    "x = torch.empty(1, requires_grad=True)\n",
    "print(\"empty x: \", x)\n",
    "normal_(x)\n",
    "print(\"random x: \", x)\n",
    "\n",
    "# create function to optimize\n",
    "y = 2 * x\n",
    "print(\"y: \",y)\n",
    "\n",
    "# calculate the gradient and feed it back\n",
    "print(\"gradient of x before backward: \", x.grad)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"gradient of x after backward: \", x.grad)\n",
    "\n",
    "# optimize with backprop information\n",
    "optimizer = optim.Adam([x], lr=0.1)\n",
    "\n",
    "y = 2 * x\n",
    "\n",
    "\n",
    "optimizer.zero_grad()\n",
    "y.backward()\n",
    "\n",
    "print(\"x before optimize: \", x)\n",
    "print(\"y: \", 2 * x)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"x after optimize: \", x)\n",
    "print(\"y: \", 2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "- torch.Tensor.backward() transfer the gradient information to the variable(weights).\n",
    "- torch.nn.optim.Adam() update the variable(weights) based on transfer the gradient information.\n",
    "\n",
    "Hence we use this train a model as following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch parameter training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. basic train\n",
    "# data:\n",
    "datagen = zip([torch.rand(1, 4) for _ in range(12)], [torch.zeros(1) for _ in range(12)])\n",
    "\n",
    "# variable(weights)\n",
    "W = Variable(torch.randn(4, 1), requires_grad=True)\n",
    "b = Variable(torch.randn(1), requires_grad=True)\n",
    "model = lambda x: torch.matmul(x, W) + b\n",
    "\n",
    "optimizer = optim.Adam([W, b])\n",
    "\n",
    "for x, y in datagen:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred = model(x)\n",
    "    loss = (pred - y) ** 2\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize variables(or tensors, or parameters in model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# Normal: (just like before)\n",
    "x = torch.empty(5, 3, requires_grad=True)\n",
    "print(\"empty\", x)\n",
    "\n",
    "xavier_uniform_(x)\n",
    "print(\"initialized\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty(single): tensor([-3.0596e+13], requires_grad=True)\n",
      "empty: tensor([[-3.0596e+13,  4.5577e-41,  2.8648e-36],\n",
      "        [ 0.0000e+00,  4.4842e-44,  0.0000e+00],\n",
      "        [ 1.5695e-43,  0.0000e+00,  2.8642e-36],\n",
      "        [ 0.0000e+00,  7.7052e+31,  7.2148e+22],\n",
      "        [ 2.5226e-18,  1.0372e-08,  1.0470e-11]])\n",
      "zeros: tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "rand: tensor([[0.3606, 0.0407, 0.1474],\n",
      "        [0.1027, 0.4070, 0.7913],\n",
      "        [0.2934, 0.8099, 0.8565],\n",
      "        [0.9631, 0.2912, 0.1615],\n",
      "        [0.7747, 0.2285, 0.4873]])\n",
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Direct:（if want it as variable, set requires_grad=True）\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(\"zeros\", x)\n",
    "\n",
    "x = torch.ones(5, 3, dtype=torch.long)\n",
    "print(\"ones\", x)\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(\"rand\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indirect:\n",
    "# from numpy\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "print(b.numpy())\n",
    "\n",
    "# from torch tensor\n",
    "x = torch.randn_like(b, dtype=torch.float)  # override dtype!\n",
    "print(x)\n",
    "\n",
    "x = x.new_ones(5, 3)      # new_* methods take in sizes\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "x2 = torch.randn(4, 4)\n",
    "x3 = torch.randn(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### operation: self-operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "[[-1.0876321   0.31195012  0.9559226  -0.77233636]\n",
      " [-0.77088886  0.33271268 -1.6035181   0.6393377 ]\n",
      " [ 0.8281358   1.2581091   0.69183874  0.4478285 ]\n",
      " [ 0.6002358   0.55723464 -0.53133804  0.19073093]]\n"
     ]
    }
   ],
   "source": [
    "y = x.view(16)    # : reshape\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "\n",
    "# get size\n",
    "print(x.size(), y.size(), z.size())\n",
    "\n",
    "# get value\n",
    "print(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1 0]\n",
      "[-0.59209573 -1.4023566   3.225912    0.81686336]\n"
     ]
    }
   ],
   "source": [
    "print(x.argmax(1).numpy())\n",
    "print(x.sum(1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### operation: two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2781, 1.2587, 0.3948],\n",
      "        [0.3611, 1.3786, 1.5138],\n",
      "        [1.3472, 1.1214, 1.0918],\n",
      "        [0.6161, 1.2757, 0.5494],\n",
      "        [0.4003, 1.6258, 1.7759]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.add(x,x2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.matmul(x3, x3.T)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "print(x == x)\n",
    "print(x == x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MmBackward object at 0x7fcc13e8a128>\n",
      "tensor([[2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x3 = torch.randn(3, 2)\n",
    "y = torch.matmul(x3, x3.T)\n",
    "\n",
    "print(y.grad_fn)\n",
    "y.backward()\n",
    "print(x3.grad)   # grad is more like delta x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congyutf",
   "language": "python",
   "name": "congyutf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
